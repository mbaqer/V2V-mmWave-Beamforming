{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-Up-Table-Revised - Scenario 36 - 64 Beams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bed759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "import utm\n",
    "import shutil\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scipyio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_name(scen_idx, n_beams, norm_type, noise):\n",
    "    return f'scenario {scen_idx} beams {n_beams} norm {norm_type} noise {noise}'\n",
    "\n",
    "\n",
    "def min_max(arr, ax=0):\n",
    "    \"\"\" Computes min-max normalization of array <arr>. \"\"\"\n",
    "    return (arr - arr.min(axis=ax)) / (arr.max(axis=ax) - arr.min(axis=ax))\n",
    "\n",
    "\n",
    "def xy_from_latlong(lat_long):\n",
    "    \"\"\" Assumes lat and long along row. Returns same row vec/matrix on\n",
    "    cartesian coords.\"\"\"\n",
    "    # utm.from_latlon() returns: (EASTING, NORTHING, ZONE_NUMBER, ZONE_LETTER)\n",
    "    x, y, *_ = utm.from_latlon(lat_long[:,0], lat_long[:,1])\n",
    "    return np.stack((x,y), axis=1)\n",
    "\n",
    "\n",
    "def add_pos_noise(pos, noise_variance_in_m=1):\n",
    "\n",
    "    n_samples = pos.shape[0]\n",
    "\n",
    "    # Get noise in xy coordinates\n",
    "    dist = np.random.normal(0, noise_variance_in_m, n_samples)\n",
    "    ang = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    xy_noise = np.stack((dist * np.cos(ang), dist * np.sin(ang)), axis=1)\n",
    "\n",
    "    # Get position in xy coordinates\n",
    "    x, y, zn, zl = utm.from_latlon(pos[:,0], pos[:,1])\n",
    "    xy_pos = np.stack((x,y), axis=1)\n",
    "\n",
    "    # Apply noise to position and return conversion to lat_long coordinates\n",
    "    xy_pos_noise = xy_pos + xy_noise\n",
    "\n",
    "    lat,long = utm.to_latlon(xy_pos_noise[:,0], xy_pos_noise[:,1], zn, zl)\n",
    "    pos_with_noise = np.stack((lat,long), axis=1)\n",
    "    return pos_with_noise\n",
    "\n",
    "\n",
    "def normalize_pos(pos1, pos2, norm_type):\n",
    "    if norm_type == 1:\n",
    "        pos_norm = min_max(pos2)\n",
    "\n",
    "    if norm_type == 2:\n",
    "        # Check where the BS is and flip axis\n",
    "        pos_norm = min_max(pos2)\n",
    "\n",
    "        avg_pos2 = np.mean(pos2, axis=0)\n",
    "\n",
    "        if pos1[0,0] > avg_pos2[0]:\n",
    "            pos_norm[:,0] = 1 - pos_norm[:,0]\n",
    "        if pos1[0,1] > avg_pos2[1]:\n",
    "            pos_norm[:,1] = 1 - pos_norm[:,1]\n",
    "\n",
    "    if norm_type == 3:\n",
    "        pos_norm = min_max(xy_from_latlong(pos2))\n",
    "\n",
    "\n",
    "    if norm_type  == 4:\n",
    "        # For relative positions, rotate axis, and min_max it.\n",
    "        pos2_cart = xy_from_latlong(pos2)\n",
    "        pos_bs_cart = xy_from_latlong(pos1)\n",
    "        avg_pos2 = np.mean(pos2_cart, axis=0)\n",
    "\n",
    "        vect_bs_to_ue = avg_pos2 - pos_bs_cart\n",
    "\n",
    "        theta = np.arctan2(vect_bs_to_ue[1], vect_bs_to_ue[0])\n",
    "        rot_matrix = np.array([[ np.cos(theta), np.sin(theta)],\n",
    "                               [-np.sin(theta), np.cos(theta)]])\n",
    "        pos_transformed =  np.dot(rot_matrix, pos2.T).T\n",
    "        pos_norm = min_max(pos_transformed)\n",
    "\n",
    "\n",
    "    if norm_type == 5:\n",
    "        pos2_cart = xy_from_latlong(pos2)\n",
    "        pos_bs_cart = xy_from_latlong(pos1)\n",
    "        pos_diff = pos2_cart - pos_bs_cart\n",
    "\n",
    "        # get distances and angle from the transformed position\n",
    "        dist = np.linalg.norm(pos_diff, axis=1)\n",
    "        ang = np.arctan2(pos_diff[:,1], pos_diff[:,0])\n",
    "\n",
    "        # Normalize distance + normalize and offset angle\n",
    "        dist_norm = dist / max(dist)\n",
    "\n",
    "        # 1- Get the angle to the average position\n",
    "        avg_pos = np.mean(pos_diff, axis=0)\n",
    "        avg_pos_ang = np.arctan2(avg_pos[1], avg_pos[0])\n",
    "\n",
    "        # A small transformation to the angle to avoid having breaks\n",
    "        # between -pi and pi\n",
    "        ang2 = np.zeros(ang.shape)\n",
    "        for i in range(len(ang)):\n",
    "            ang2[i] = ang[i] if ang[i] > 0 else ang[i] + 2 * np.pi\n",
    "\n",
    "        avg_pos_ang2 = \\\n",
    "            avg_pos_ang + 2 * np.pi if avg_pos_ang < 0 else avg_pos_ang\n",
    "\n",
    "        # 2- Offset angle avg position at 90ยบ\n",
    "        offset2 = np.pi/2 - avg_pos_ang2\n",
    "        ang_final = ang2 + offset2\n",
    "\n",
    "        # MAP VALUES OF 0-PI TO 0-1\n",
    "        ang_norm = ang_final / np.pi\n",
    "\n",
    "        pos_norm = np.stack((dist_norm,ang_norm), axis=1)\n",
    "\n",
    "    return pos_norm\n",
    "\n",
    "\n",
    "def mode_list(arr):\n",
    "    \"\"\" Returns ordered list based on # of occurences in 1D array. \"\"\"\n",
    "    vals, counts = np.unique(arr, return_counts=True)\n",
    "    return vals[np.flip(np.argsort(counts))]\n",
    "\n",
    "\n",
    "def pos_to_bin(pos, bin_size, n_bins):\n",
    "    # The bin indices will be flattened out\n",
    "    #\n",
    "    # x2\n",
    "    # ^\n",
    "    # | d e f\n",
    "    # | a b c\n",
    "    # --------> x1\n",
    "    #\n",
    "    # Will be mapped to: a b c d e f\n",
    "    if pos[0] == 1:\n",
    "        pos[0] -= 1e-9\n",
    "\n",
    "    if pos[1] == 1:\n",
    "        pos[1] -= 1e-9\n",
    "\n",
    "    bin_idx = int(np.floor(pos[0] / bin_size[0]) +\n",
    "                  1 / bin_size[0] * np.floor(pos[1] / bin_size[1]))\n",
    "\n",
    "    return max(min(bin_idx, n_bins-1), 0)\n",
    "\n",
    "\n",
    "def write_results_together(ai_strategy, top_beams, runs_folder, n_runs,\n",
    "                           val_accs, test_accs, mean_power_losses):\n",
    "    results_file = os.path.join(runs_folder, f'{n_runs}-runs_results_summary.txt')\n",
    "    with open(results_file, 'w') as fp:\n",
    "        fp.write('Test Results: \\n')\n",
    "        # For test accuracy results\n",
    "        for i in range(len(top_beams)):\n",
    "            s = f'Top-{top_beams[i]} average accuracy ' + \\\n",
    "                 f'{np.mean(test_accs[:,i]):.2f} % '\n",
    "            print(s, end='')\n",
    "            fp.write(s)\n",
    "        fp.write('\\n')\n",
    "        # For test Power loss results.\n",
    "        fp.write('Power Loss results\\n')\n",
    "        fp.write(f\"Mean:{np.mean(mean_power_losses):.2f}, \"\n",
    "                 f\"STD: {np.std(mean_power_losses):.4f} \")\n",
    "\n",
    "\n",
    "def write_results_separate(top_beams, results_folder, n_runs,\n",
    "                           val_accs, test_accs, mean_power_losses):\n",
    "    variables = [val_accs, test_accs, mean_power_losses]\n",
    "\n",
    "    for idx, var in enumerate(variables):\n",
    "        if idx != 2: # (power loss doesn't have top-X results)\n",
    "            for i, top_beam in enumerate(top_beams):\n",
    "                mean_and_std = np.array([np.mean(var[:,i]), np.std(var[:,i])])\n",
    "                acc_str = 'val' if idx == 0 else 'test'\n",
    "                fname = os.path.join(results_folder,\n",
    "                                     f'{n_runs}-runs_top-{top_beam}_'\n",
    "                                     f'{acc_str}_acc.txt',)\n",
    "                np.savetxt(fname, mean_and_std, fmt='%.2f')\n",
    "        else:\n",
    "            mean_and_std = np.array([np.mean(var), np.std(var)])\n",
    "            fname = os.path.join(results_folder,\n",
    "                                 f'{n_runs}-runs_mean_power_loss_db.txt')\n",
    "            np.savetxt(fname, mean_and_std, fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where every result folder will be saved too:\n",
    "save_folder =  os.path.join(os.getcwd(), f'saved_folder/results_{time.time()}')\n",
    "\n",
    "# Variables to loop\n",
    "norm_types = [1]                     # [1,2,3,4,5]\n",
    "scen_idxs = [1] #np.arange(1,9+1)   # [1,2,3,4,5,6,7,9]\n",
    "n_beams_list = [64]          # [8, 16,32,64]\n",
    "noises = [0]                         # position noise in meters\n",
    "n_reps = 5 #5                           # number repetitions of current settings.\n",
    "\n",
    "# Variables constant across simulation\n",
    "max_samples = 1e9                  # max samples to consider per scenario\n",
    "n_avgs = 5 # 5                            # number of runs to average\n",
    "train_val_test_split = [60,20,20]     # 0 on val uses test set to validate.\n",
    "top_beams = np.arange(15) + 1          # Collect stats for Top X predictions\n",
    "force_seed = -1                       # When >= 0, sets data randimzation\n",
    "                                      # seed. Useful for data probing.\n",
    "                                      # Otherwise, seed = run_idx.\n",
    "# Hyperparameters:\n",
    "n_lookuptable = 25                    # number of divisions of each coordinate\n",
    "use_best_n_lookuptable = False         # if True, ignores the value above.\n",
    "BEST_N_PER_SCENARIO_TAB = \\\n",
    "    [62,40,27,22,30,33,27,20,27]      # best n measured in each scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = list(itertools.product(scen_idxs, n_beams_list, norm_types,\n",
    "                                      noises, [1 for i in range(n_reps)]))\n",
    "\n",
    "for scen_idx, n_beams, norm_type, noise, rep in combinations:\n",
    "\n",
    "    print(f'Executing for scen={scen_idx}, beams={n_beams}, norm={norm_type}')\n",
    "\n",
    "    # data_folder = os.path.join(os.getcwd(), f'Ready_data_norm{norm_type}')\n",
    "    \n",
    "    data_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "    # The saved folder will have all experiments conducted.\n",
    "    experiment_name = get_experiment_name(scen_idx, n_beams, norm_type, noise)\n",
    "    saved_path = os.path.join(save_folder, experiment_name)\n",
    "\n",
    "\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    if not os.path.isdir(saved_path):\n",
    "        os.mkdir(saved_path)\n",
    "\n",
    "    # ----------------------- Phase 1: Data Loading ---------------------------\n",
    "\n",
    "    ##\n",
    "    \n",
    "    csv_file_path = os.path.abspath('scenario36_64_pos_beam.csv')\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    pos1_paths = np.array(data[\"original_unit1_gps1\"])\n",
    "    pos2_paths = np.array(data[\"original_unit2_gps1\"])\n",
    "    pwr1_paths = np.array(data[\"original_unit1_pwr1\"])\n",
    "\n",
    "    # pos1\n",
    "    pos1 = []\n",
    "    for path in pos1_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', path.split('/', 1)[1])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                pos1.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    pos1 = np.array(pos1)\n",
    "\n",
    "    # pos2\n",
    "    pos2 = []\n",
    "    for path in pos2_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', 'unit2', path.split('/', 2)[2])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                pos2.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    pos2 = np.array(pos2)\n",
    "    \n",
    "    # pwr1\n",
    "    pwr1 = []\n",
    "    for path in pwr1_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', 'unit1', path.split('/', 2)[2])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                pwr1.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    pwr1 = np.array(pwr1)\n",
    "\n",
    "    print(pwr1.shape)\n",
    "    ##\n",
    "\n",
    "    max_beams = pwr1.shape[-1]\n",
    "\n",
    "    n_samples = min(len(pos2), max_samples)\n",
    "    \n",
    "   # ========= Phase *: Using csv files for val, train, test data  =============\n",
    "    data_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    \n",
    "    origin_data_path = os.path.join(data_folder, 'Scenario36', 'scenario36.csv')\n",
    "    \n",
    "    test_data_path = os.path.abspath('scenario36_64_pos_beam_test.csv')\n",
    "    train_data_path = os.path.abspath('scenario36_64_pos_beam_train.csv')\n",
    "    val_data_path = os.path.abspath('scenario36_64_pos_beam_val.csv')\n",
    "    \n",
    "    \n",
    "    origin_data = pd.read_csv(origin_data_path)\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    \n",
    "    \n",
    "    origin_index = np.array(origin_data[\"abs_index\"])\n",
    "\n",
    "    test_index = np.array(test_data[\"original_index\"])\n",
    "    train_index = np.array(train_data[\"original_index\"])\n",
    "    val_index = np.array(val_data[\"original_index\"])\n",
    "    \n",
    "    \n",
    "    #=========- updating indices of index array because original index is not continuous =========\n",
    "    # convert index array into list\n",
    "    origin_index_lst = list(origin_index)\n",
    "    \n",
    "    # test\n",
    "    for i in range(len(test_index)):\n",
    "        test_index[i] = origin_index_lst.index(test_index[i])\n",
    "    \n",
    "    # train\n",
    "    for i in range(len(train_index)):\n",
    "        train_index[i] = origin_index_lst.index(train_index[i])\n",
    "    \n",
    "    # val\n",
    "    for i in range(len(val_index)):\n",
    "        val_index[i] = origin_index_lst.index(val_index[i])\n",
    "\n",
    "    #=============================================================================================\n",
    "    \n",
    "    \n",
    "    test_pwr1_paths = np.array(test_data[\"original_unit1_pwr1\"])\n",
    "    train_pwr1_paths = np.array(train_data[\"original_unit1_pwr1\"])\n",
    "    val_pwr1_paths = np.array(val_data[\"original_unit1_pwr1\"])\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    \n",
    "    test_pwr1 = []\n",
    "    for path in test_pwr1_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', 'unit1', path.split('/', 2)[2])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                test_pwr1.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    test_pwr1 = np.array(test_pwr1)\n",
    "    top_test_data = np.argmax(test_pwr1, axis=1)\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    train_pwr1 = []\n",
    "    for path in train_pwr1_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', 'unit1', path.split('/', 2)[2])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                train_pwr1.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    train_pwr1 = np.array(train_pwr1)\n",
    "    top_train_data = np.argmax(train_pwr1, axis=1)\n",
    "    \n",
    "    # val\n",
    "    \n",
    "    val_pwr1 = []\n",
    "    for path in val_pwr1_paths:\n",
    "        file_path = os.path.join(data_folder, 'Scenario36', 'unit1', path.split('/', 2)[2])\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                val_pwr1.append([float(line.strip()) for line in file])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    val_pwr1 = np.array(val_pwr1)\n",
    "    top_val_data = np.argmax(val_pwr1, axis=1)\n",
    "\n",
    "    # -------------------- Phase 2: Data Preprocessing ------------------------\n",
    "\n",
    "    # Trim altitudes if they exists\n",
    "    pos1 = pos1[:,:2]\n",
    "    pos2 = pos2[:,:2]\n",
    "\n",
    "    # Insert Noise if enabled.\n",
    "    pos2_with_noise = add_pos_noise(pos2, noise_variance_in_m=noise)\n",
    "\n",
    "    # Normalize position\n",
    "    pos_norm = normalize_pos(pos1, pos2_with_noise, norm_type)\n",
    "\n",
    "    # Assign beam values (and automatically downsample if n_beams != 64)\n",
    "    if n_beams not in [8, 16, 32, 64]:\n",
    "        raise Exception('')\n",
    "\n",
    "    divider = max_beams // n_beams\n",
    "    beam_idxs = np.arange(0, max_beams, divider)\n",
    "\n",
    "    # Select alternating samples (every 2, 4 or 8)\n",
    "    beam_pwrs = pwr1[:,beam_idxs]\n",
    "    # Convert beam indices. 32 beams: ([0,2,4,..., 62]) -> [0,1,2,... 32]\n",
    "    beam_data = np.argmax(beam_pwrs, axis=1)\n",
    "\n",
    "    # ----------------- Phase 3: Define Path for run --------------------------\n",
    "\n",
    "    # We first define the folder where results from this run will be saved\n",
    "    # In that folder there will be other runs too, and that will tell us what's\n",
    "    # the index of this run. That information is used to shuffle the data\n",
    "    # in a reproducible way. Run 1 uses seed 1, run 2 uses seed 2, etc.\n",
    "\n",
    "    # Compute parameters needed for setting runs_folder name\n",
    "    # The Runs Folder contains the folders of each run.\n",
    "\n",
    "    if use_best_n_lookuptable:\n",
    "        n = BEST_N_PER_SCENARIO_TAB[scen_idx-1]\n",
    "    else:\n",
    "        n = n_lookuptable\n",
    "\n",
    "    runs_folder_name = f'LT_N={n}'\n",
    "    runs_folder = os.path.join(saved_path, runs_folder_name)\n",
    "\n",
    "    # Create if doesn't exist\n",
    "    if not os.path.isdir(runs_folder):\n",
    "        os.mkdir(runs_folder)\n",
    "\n",
    "    # Experiment index: number of experiments already conducted + 1\n",
    "    run_idx = 1 + sum(os.path.isdir(os.path.join(runs_folder, run_folder))\n",
    "                        for run_folder in os.listdir(runs_folder))\n",
    "\n",
    "    now_time = datetime.datetime.now().strftime('Time_%m-%d-%Y_%Hh-%Mm-%Ss')\n",
    "\n",
    "    run_folder = os.path.join(runs_folder, f\"{run_idx}-{now_time}\")\n",
    "\n",
    "    # Check if there are enough runs. If yes, skip data loading, model\n",
    "    # training and testing, and jump to averaging the results.\n",
    "    \n",
    "    if run_idx > n_avgs:\n",
    "        print('Already enough experiments conducted for '\n",
    "                'this case. Either increase n_avgs, or try '\n",
    "                'a different set of parameters. SKIPPING TO the avg. '\n",
    "                'computation!')\n",
    "    else:\n",
    "        # -------------------- Phase 4: Split Data ------------------------\n",
    "\n",
    "        # Create folder for the run\n",
    "        os.mkdir(run_folder)\n",
    "\n",
    "        # Shuffle Data (fixed, for reproducibility)\n",
    "        if force_seed >= 0:\n",
    "            np.random.seed(force_seed)\n",
    "        else:\n",
    "            np.random.seed(run_idx)\n",
    "        sample_shuffle_list = np.random.permutation(n_samples)\n",
    "\n",
    "        # Select sample indices for each set\n",
    "        first_test_sample = int((1-train_val_test_split[2]) / 100 * n_samples)\n",
    "        train_val_samples = sample_shuffle_list[:first_test_sample]\n",
    "        test_samples = sample_shuffle_list[first_test_sample:]\n",
    "\n",
    "        # (We have no use for x_val in KNN or LookupTable.)\n",
    "        # CHOICE: we used train+val(80%) sets to train in KNN and the LUtable\n",
    "        #         but in the NN we used only train(60%). This seemed a fair\n",
    "        #         approach, and doing otherwise yields little difference.\n",
    "        if train_val_test_split[1] == 0:\n",
    "            train_samples = train_val_samples\n",
    "            val_samples = test_samples\n",
    "        else:\n",
    "            train_ratio = np.sum(train_val_test_split[:2]) / 100\n",
    "            first_val_sample = int(train_val_test_split[1] / 100 *\n",
    "                                    len(train_val_samples) / train_ratio)\n",
    "            val_samples = train_val_samples[:first_val_sample]\n",
    "            train_samples = train_val_samples[first_val_sample:]\n",
    "\n",
    "        x_train = pos_norm[train_samples]\n",
    "        y_train = beam_data[train_samples]\n",
    "        x_val = pos_norm[val_samples]\n",
    "        y_val = beam_data[val_samples]\n",
    "        x_test = pos_norm[test_samples]\n",
    "        y_test = beam_data[test_samples]\n",
    "        y_test_pwr = beam_pwrs[test_samples]\n",
    "         \n",
    "        \n",
    "        #=========== updated code by csv file ================\n",
    "        \n",
    "        new_x_train = pos_norm[train_index]\n",
    "        new_y_train = top_train_data\n",
    "        new_x_val = pos_norm[val_index]\n",
    "        new_y_val = top_val_data\n",
    "        new_x_test = pos_norm[test_index]\n",
    "        new_y_test = top_test_data\n",
    "        new_y_test_pwr = beam_pwrs[test_samples]\n",
    "        \n",
    "        #======================================================\n",
    "\n",
    "    # ---------------------- Phase 5: Train & Test ------------------------\n",
    "\n",
    "        # Useful little variables\n",
    "        n_test_samples = len(new_x_test)\n",
    "        n_top_stats = len(top_beams)\n",
    "\n",
    "        # Variables for compatibility (when not all predictors are used)\n",
    "        n_bins, bin_size, prediction_per_bin = None, None, None\n",
    "        trained_model = None\n",
    "\n",
    "\n",
    "    if run_idx <= n_avgs:\n",
    "\n",
    "        # 1- Define bins\n",
    "\n",
    "        n_bins_across_x1 = 8\n",
    "        n_bins_across_x2 = 8\n",
    "        bin_size = np.array([1,1]) / [n_bins_across_x1, n_bins_across_x2]\n",
    "        n_bins = n_bins_across_x1 * n_bins_across_x2\n",
    "#         n_bins = 65\n",
    "\n",
    "        # 2- Create a list with the samples per bin\n",
    "        samples_per_bin = [[] for bin_idx in range(n_bins)]\n",
    "\n",
    "        # 3- Map each input to a bin\n",
    "        for x_idx, x in enumerate(new_x_train):\n",
    "            samples_per_bin[pos_to_bin(x, bin_size, n_bins)].append(x_idx)\n",
    "\n",
    "        # 4- Define the values to predict for samples in that bin\n",
    "        prediction_per_bin = [mode_list(new_y_train[samples_per_bin[bin_idx]])\n",
    "                                for bin_idx in range(n_bins)]\n",
    "\n",
    "        \n",
    "        # 5- Evaluation phase. Map each test sample to a bin and get the\n",
    "        #    prediction.\n",
    "        pred_beams = []\n",
    "        for x in x_test:\n",
    "            pred = prediction_per_bin[pos_to_bin(x, bin_size, n_bins)]\n",
    "            if pred.size == 0:\n",
    "                pred = [int(np.random.uniform(0, n_beams))]\n",
    "            pred_beams.append(np.asarray(pred))\n",
    "            \n",
    "        pred_values = []\n",
    "        for x in new_x_test:\n",
    "            pred = prediction_per_bin[pos_to_bin(x, bin_size, n_bins)]\n",
    "            if pred.size == 0:\n",
    "                pred = [int(np.random.uniform(0, n_beams))]\n",
    "            pred_values.append(np.asarray(pred))\n",
    "        print(len(pred_values[0]))\n",
    "\n",
    "    #=========== Add Top-1 ~ Top-15 columns into test csv file ===========\n",
    "        # making only Top-1, Top-3, ... array\n",
    "        top_1_data = [each_data[0] for each_data in pred_values]\n",
    "        top_3_data = [each_data if len(each_data) < 3 else each_data[:3] for each_data in pred_values]\n",
    "        top_5_data = [each_data if len(each_data) < 5 else each_data[:5] for each_data in pred_values]\n",
    "        top_7_data = [each_data if len(each_data) < 7 else each_data[:7] for each_data in pred_values]\n",
    "        top_9_data = [each_data if len(each_data) < 9 else each_data[:9] for each_data in pred_values]\n",
    "        top_11_data = [each_data if len(each_data) < 11 else each_data[:11] for each_data in pred_values]\n",
    "        top_13_data = [each_data if len(each_data) < 13 else each_data[:13] for each_data in pred_values]\n",
    "        top_15_data = [each_data if len(each_data) < 15 else each_data[:15] for each_data in pred_values]\n",
    "        \n",
    "        \n",
    "        # Add Top-1, Top-3,... columns\n",
    "        test_data[\"Top-1\"] = \"\"\n",
    "        test_data[\"Top-1\"] = top_1_data\n",
    "        \n",
    "        test_data[\"Top-3\"] = \"\"\n",
    "        test_data[\"Top-3\"] = top_3_data\n",
    "        \n",
    "        test_data[\"Top-5\"] = \"\"\n",
    "        test_data[\"Top-5\"] = top_5_data\n",
    "        \n",
    "        test_data[\"Top-7\"] = \"\"\n",
    "        test_data[\"Top-7\"] = top_7_data\n",
    "        \n",
    "        test_data[\"Top-9\"] = \"\"\n",
    "        test_data[\"Top-9\"] = top_9_data\n",
    "        \n",
    "        test_data[\"Top-11\"] = \"\"\n",
    "        test_data[\"Top-11\"] = top_11_data\n",
    "        \n",
    "        test_data[\"Top-13\"] = \"\"\n",
    "        test_data[\"Top-13\"] = top_13_data\n",
    "        \n",
    "        test_data[\"Top-15\"] = \"\"\n",
    "        test_data[\"Top-15\"] = top_15_data\n",
    "        \n",
    "        # update test csv file\n",
    "        test_data.to_csv(test_data_path, index=False)\n",
    "    \n",
    "    #=====================================================================\n",
    "    \n",
    "    \n",
    "    # ----------- Phase 6: Save Accuracies and Power Losses ---------------\n",
    "\n",
    "        # Get top-1, top-2, top-3 and top-5 accuracies\n",
    "        total_hits = np.zeros(n_top_stats)\n",
    "\n",
    "        # For each test sample, count times where true beam is in top 1,2,3,5\n",
    "        for i in range(n_test_samples):\n",
    "            for j in range(n_top_stats):\n",
    "                hit = np.any(pred_beams[i][:top_beams[j]] == y_test[i])\n",
    "                total_hits[j] += 1 if hit else 0\n",
    "\n",
    "        # Average the number of correct guesses (over the total samples)\n",
    "        acc = np.round(total_hits / n_test_samples, 4)\n",
    "\n",
    "        print(f'LT Results:')\n",
    "        for i in range(n_top_stats):\n",
    "            print(f'\\tAverage Top-{top_beams[i]} accuracy {acc[i]*100:.2f}')\n",
    "\n",
    "        # Save Test acc to file\n",
    "        np.savetxt(os.path.join(run_folder, 'test_accs.txt'),\n",
    "                    acc * 100, fmt='%.2f')\n",
    "\n",
    "        # We consider the noise per sample, not per scenario:\n",
    "        # Noise is the lowest power in each sample.\n",
    "        power_loss_ratio = np.zeros(n_test_samples)\n",
    "        for i in range(n_test_samples):\n",
    "            noise = np.min(y_test_pwr[i,:]) / 2\n",
    "\n",
    "            pred_pwr = y_test_pwr[i,pred_beams[i][0]]\n",
    "            true_pwr = np.max(y_test_pwr[i,:])\n",
    "\n",
    "            if pred_pwr == noise:\n",
    "                # In Lookup table it may be the case we predict the worst\n",
    "                # beam to be the best. In this case, adjust noise slightly\n",
    "                # just to avoid -inf dB loss. This is extremely rare and\n",
    "                # will not affect results noticeably.\n",
    "                noise = noise/2\n",
    "\n",
    "            power_loss_ratio[i] = ((true_pwr - noise) /\n",
    "                                    (pred_pwr - noise))\n",
    "\n",
    "        mean_power_loss_db = 10 * np.log10(np.mean(power_loss_ratio))\n",
    "\n",
    "        print(f\"{mean_power_loss_db:.4f}\")\n",
    "\n",
    "        np.savetxt(os.path.join(run_folder, 'power_loss.txt'),\n",
    "                    np.stack((mean_power_loss_db, 0))) # needs to be 1D..\n",
    "\n",
    "\n",
    "\n",
    "        # -------------- Phase 7: Compute average across runs ----------------\n",
    "    if run_idx >= n_avgs:\n",
    "        folders_of_each_run = [os.path.join(runs_folder, folder)\n",
    "                                for folder in os.listdir(runs_folder)]\n",
    "\n",
    "        folders_of_each_run = [folder for folder in folders_of_each_run\n",
    "                                if os.path.isdir(folder)]\n",
    "\n",
    "        n_run_folders = len(folders_of_each_run)\n",
    "        val_accs = np.zeros((n_run_folders, len(top_beams)))\n",
    "        test_accs = np.zeros((n_run_folders, len(top_beams)))\n",
    "        mean_power_losses = np.zeros(n_run_folders)\n",
    "        for run_idx in range(n_run_folders):\n",
    "            run_folder = folders_of_each_run[run_idx]\n",
    "            test_accs_file = os.path.join(run_folder, 'test_accs.txt')\n",
    "            pwr_loss_file = os.path.join(run_folder, 'power_loss.txt')\n",
    "\n",
    "            test_accs[run_idx] = np.loadtxt(test_accs_file)\n",
    "            mean_power_losses[run_idx] = np.loadtxt(pwr_loss_file)[0]\n",
    "\n",
    "        print(f'Computing the average of {n_run_folders} runs. ')\n",
    "\n",
    "        # Write results to same text file\n",
    "        write_results_together('LT', top_beams, runs_folder,\n",
    "                                    n_run_folders, val_accs,\n",
    "                                    test_accs, mean_power_losses)\n",
    "\n",
    "        write_results_separate(top_beams, runs_folder, n_run_folders,\n",
    "                                    val_accs, test_accs, mean_power_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
