{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d31ef-a4f8-463c-b487-9895cb0d12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2V Scenario 36 - 64 Beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d78de-1fb4-413c-80bb-e7846cf9d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transf\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import ast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d45a85-f611-4499-9299-d6fbe6629772",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Create save directory\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bce3f-91dd-4e86-b13c-5090644732f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year month day \n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds \n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec594e6-117b-43a1-af7e-8f2335c7ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime \n",
    "print(pwd)\n",
    "# Determine whether the folder already exists\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd2509-991e-4409-8a2e-2dcdcb1833b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training files to the saved directory\n",
    "shutil.copy('./scenario36_64_pos_beam_train.csv', pwd)\n",
    "shutil.copy('./scenario36_64_pos_beam_val.csv', pwd)\n",
    "shutil.copy('./scenario36_64_pos_beam_test.csv', pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3685c-e6eb-44a2-9339-0149351a206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save analysis files and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59690c48-e4cd-42e2-8073-de83ec542de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory) \n",
    "        \n",
    "isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9a371-7b63-4559-b9b3-e378624a2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Data Feeding: Create data sample list\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92922cd3-b25f-4c67-9062-d39155a4f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    pred_val = []\n",
    "    for idx, row in f.iterrows():\n",
    "        data = list(row.values[1:])\n",
    "        data_samples.append(data)\n",
    "\n",
    "    return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454ae1a-5492-4ea1-85b0-68c0ba8ee371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFeed(Dataset):\n",
    "    '''\n",
    "    A class retrieving a tuple of (image,label). It can handle the case\n",
    "    of empty classes (empty folders).\n",
    "    '''\n",
    "    def __init__(self,root_dir, nat_sort = False, transform=None, init_shuflle = True):\n",
    "        self.root = root_dir\n",
    "        self.samples = create_samples(self.root,shuffle=init_shuflle,nat_sort=nat_sort)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.samples )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        pos_val = sample[:1]\n",
    "        pos_val = ast.literal_eval(pos_val[0])    \n",
    "        pos_val = np.asarray(pos_val)\n",
    "\n",
    "        pos_centers = sample[1:2]\n",
    "        pos_centers = np.asarray(pos_centers)\n",
    "\n",
    "        return (pos_val, pos_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cae3f-6fdc-4beb-9576-c91faa69a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Training, Testing, and Validation!\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288aa7ad-fb0c-4cd5-b11e-9b9c177f9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0248c-88d4-4df6-90f9-295f068a841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-parameters\n",
    "batch_size = 128\n",
    "val_batch_size = 1\n",
    "lr = 0.01\n",
    "decay = 1e-4\n",
    "num_epochs = 30\n",
    "train_size = [1]\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "node = 512\n",
    "output_size = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722ff51-da0e-4bd6-9bc6-28222262d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8a08e-86f9-44f7-9e0d-2c0e7adeaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_pipe = transf.Compose(\n",
    "    [\n",
    "        transf.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dir = './scenario36_64_pos_beam_train.csv'\n",
    "val_dir = './scenario36_64_pos_beam_val.csv'\n",
    "\n",
    "train_loader = DataLoader(DataFeed(train_dir, transform = proc_pipe),\n",
    "                            batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                            shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(val_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3cceb-d66b-4864-957c-4ea7d7fac41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Main Model\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61686e1-be55-450a-b3f2-f7a3ccca30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_beam_pred(nn.Module):\n",
    "    def __init__(self, num_features, num_output):\n",
    "        super(NN_beam_pred, self).__init__()\n",
    "            \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining the convolution layer\n",
    "            nn.Conv1d(num_features, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(10, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(10, 30, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(30, 20, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv1d(20, 10, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "      \n",
    "        )\n",
    "        \n",
    "        self.l1 = nn.Linear(20, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, num_output)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.cnn_layers(inputs)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4264b1-8a20-4e28-b5da-3c452d2f6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "model = NN_beam_pred(input_size, num_output=output_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bc186-23a7-48da-9ed5-abb1d65c08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "\n",
    "with cuda.device(0):\n",
    "    top_1 = np.zeros( (1,len(train_size)) )\n",
    "    top_3 = np.zeros( (1,len(train_size)) )\n",
    "    top_5 = np.zeros( (1,len(train_size)) )\n",
    "    top_7 = np.zeros( (1,len(train_size)) )\n",
    "    top_9 = np.zeros( (1,len(train_size)) )\n",
    "    top_11 = np.zeros( (1,len(train_size)) )\n",
    "    top_13 = np.zeros( (1,len(train_size)) )\n",
    "    top_15 = np.zeros( (1,len(train_size)) )\n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "        # Build the network:\n",
    "        net = model.cuda()\n",
    "        layers = list(net.children())\n",
    "    \n",
    "        #  Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [15,25,40], gamma=0.1, last_epoch=-1)\n",
    "    \n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []  \n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        running_top7_acc = []\n",
    "        running_top9_acc = []\n",
    "        running_top11_acc = []\n",
    "        running_top13_acc = []\n",
    "        running_top15_acc = []\n",
    "        \n",
    "        best_accuracy = 0\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            for tr_count, (pos_data, beam_val) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                data = pos_data.type(torch.Tensor)                  \n",
    "                label = beam_val[:,0].type(torch.LongTensor)                    \n",
    "                x = data.cuda()\n",
    "                # print(\"x size\", x.size())\n",
    "                opt.zero_grad()\n",
    "                label = label.cuda()\n",
    "                # print(\"label size\", label.size())\n",
    "                # print(x.shape)\n",
    "                out = net.forward(x.reshape([-1, 2, 1]))\n",
    "                # print(out.shape, label.shape)\n",
    "                loss = criterion(out, label)\n",
    "                loss.backward()\n",
    "                opt.step()                    \n",
    "                batch_loss = loss.item()\n",
    "                acc_loss += batch_loss\n",
    "                count += 1\n",
    "                if np.mod(count, 100) == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)  # running_loss.append()\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "    \n",
    "            print('Start validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ave_top7_acc = 0\n",
    "            ave_top9_acc = 0\n",
    "            ave_top11_acc = 0\n",
    "            ave_top13_acc = 0\n",
    "            ave_top15_acc = 0\n",
    "            ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "            top1_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            top7_pred_out = []\n",
    "            top9_pred_out = []\n",
    "            top11_pred_out = []\n",
    "            top13_pred_out = []\n",
    "            top15_pred_out = []\n",
    "            total_count = 0\n",
    "\n",
    "            gt_beam = []\n",
    "            for val_count, (pos_data, beam_val) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                data = pos_data.type(torch.Tensor)  \n",
    "                x = data.cuda()                    \n",
    "                labels = beam_val[:,0].type(torch.LongTensor)   \n",
    "                opt.zero_grad()\n",
    "                labels = labels.cuda()\n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "                total_count += labels.size(0)\n",
    "                out = net.forward(x.reshape([-1, 2, 1]))\n",
    "                _, top_1_pred = t.max(out, dim=1)\n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "                sorted_out = t.argsort(out, dim=1, descending=True)\n",
    " \n",
    "                top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "        \n",
    "                top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "                \n",
    "                top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "                top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "                \n",
    "                top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "                top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "                \n",
    "                top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "                top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "                \n",
    "                top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "                top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "                \n",
    "                top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "                top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "                        \n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "                tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "                tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "                tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "                tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "                tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "                   \n",
    "                batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "                batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "                batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "                batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "                batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "                batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "                batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "                batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()\n",
    "                ave_top7_acc += batch_top7_acc.item()\n",
    "                ave_top9_acc += batch_top9_acc.item()\n",
    "                ave_top11_acc += batch_top11_acc.item()\n",
    "                ave_top13_acc += batch_top13_acc.item()\n",
    "                ave_top15_acc += batch_top15_acc.item()\n",
    "            \n",
    "            print(\"total test examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)  # (batch_size * (count_2 + 1))) \n",
    "            running_top7_acc.append(ave_top7_acc / total_count) \n",
    "            running_top9_acc.append(ave_top9_acc / total_count) \n",
    "            running_top11_acc.append(ave_top11_acc / total_count) \n",
    "            running_top13_acc.append(ave_top13_acc / total_count)\n",
    "            running_top15_acc.append(ave_top15_acc / total_count)\n",
    "            \n",
    "            print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "            print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "            print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "            print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "            print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "            print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "            \n",
    "            cur_accuracy  = running_top1_acc[-1]\n",
    "   \n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory  + '//' +  'cnn_beam_pred-64beams'\n",
    "                t.save(net.state_dict(), net_name)  \n",
    "                best_accuracy =  cur_accuracy  \n",
    "            print(\"updated best acc\", best_accuracy)\n",
    "                \n",
    "                        \n",
    "            print(\"Saving the predicted value in a csv file\")\n",
    "            file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch+1}th_epoch.csv'\n",
    "            indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "            df1 = pd.DataFrame()\n",
    "            df1['index'] = indx                \n",
    "            df1['link_status'] = gt_beam\n",
    "            df1['top1_pred'] = top1_pred_out\n",
    "            df1['top3_pred'] = top3_pred_out\n",
    "            df1['top5_pred'] = top5_pred_out\n",
    "            df1['top7_pred'] = top7_pred_out\n",
    "            df1['top9_pred'] = top9_pred_out\n",
    "            df1['top11_pred'] = top11_pred_out\n",
    "            df1['top13_pred'] = top13_pred_out\n",
    "            df1['top15_pred'] = top15_pred_out\n",
    "            df1.to_csv(file_to_save, index=False)\n",
    "                                      \n",
    "            LR_sch.step()\n",
    "            \n",
    "        top_1[0,idx] = running_top1_acc[-1]\n",
    "        top_3[0,idx] = running_top3_acc[-1]\n",
    "        top_5[0,idx] = running_top5_acc[-1]\n",
    "        top_7[0,idx] = running_top7_acc[-1]\n",
    "        top_9[0,idx] = running_top9_acc[-1]\n",
    "        top_11[0,idx] = running_top11_acc[-1]\n",
    "        top_13[0,idx] = running_top13_acc[-1]\n",
    "        top_15[0,idx] = running_top15_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b528ea-863b-4374-b977-68c0a93c21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e55e7-04fd-46f7-a605-e67f507a1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "test_dir = './scenario36_64_pos_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr1_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr1'].tolist()\n",
    "\n",
    "# Load the model checkpoint and prepare for evaluation\n",
    "checkpoint_path = f'{checkpoint_directory}/cnn_beam_pred-64beams'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval() \n",
    "net = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684870fc-a3ff-4315-8859-7ae6159fd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(DataFeed(test_dir, transform=proc_pipe),\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257bb56-9ece-4eab-a8ed-658377a9a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []  \n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, (pos_data, beam_val) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "    data = pos_data.type(torch.Tensor)  \n",
    "    x = data.cuda()                    \n",
    "    labels = beam_val[:,0].type(torch.LongTensor)   \n",
    "    opt.zero_grad()\n",
    "    labels = labels.cuda()\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x.reshape([-1, 2, 1]))\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "    \n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "            \n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "       \n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f650e-258a-4f92-9ef0-884affa3a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)   \n",
    "running_top9_acc.append(ave_top9_acc / total_count)   \n",
    "running_top11_acc.append(ave_top11_acc / total_count)   \n",
    "running_top13_acc.append(ave_top13_acc / total_count)   \n",
    "running_top15_acc.append(ave_top15_acc / total_count)   \n",
    "\n",
    "print('Testing_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data  # Add the link_status column\n",
    "df2['original_unit1_pwr1'] = pwr_60ghz # Add the original_unit1_pwr_60ghz column\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
